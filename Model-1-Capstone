{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2250642,"sourceType":"datasetVersion","datasetId":1075326}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install spacy langdetect wordcloud --no-deps\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport spacy\nimport nltk\nimport re\nimport string\nimport wordcloud\nfrom langdetect import detect\nfrom collections import Counter\nfrom nltk.corpus import stopwords, wordnet\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.corpus import words","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"markdown","source":"### Read the Data","metadata":{}},{"cell_type":"code","source":"data_path = '/kaggle/input/suicide-watch/Suicide_Detection.csv'\ndf = pd.read_csv(data_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head(20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Data Summary","metadata":{}},{"cell_type":"code","source":"print(\"\\nSummary Statistics:\")\nprint(df.describe())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Check for missing value","metadata":{}},{"cell_type":"code","source":"print(\"\\nMissing Values:\")\nprint(df.isnull().sum())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Count the class distribution","metadata":{}},{"cell_type":"code","source":"df['class'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if 'class' in df.columns:\n    sns.countplot(x='class', data=df)\n    plt.title('Distribution of Labels')\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Check for any duplicates rows","metadata":{}},{"cell_type":"code","source":"duplicate_count = df.duplicated().sum()\nprint(f\"Number of duplicate rows: {duplicate_count}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Display word cloud","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\nall_text = ' '.join(df['text'].dropna().astype(str))\n\nwordcloud = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(all_text)\n\nplt.figure(figsize=(15, 7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Word Cloud of Text Data')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"### Lowercase all the text","metadata":{}},{"cell_type":"code","source":"df['text'] = df['text'].str.lower()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head(20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Replace \"’\" with \"'\"","metadata":{}},{"cell_type":"code","source":"df['text'] = df['text'].str.replace(\"’\", \"'\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head(20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Replace the abbreviations","metadata":{}},{"cell_type":"code","source":"abb = {\n  \"ain't\": \"am not\",\n  \"aren't\": \"are not\",\n  \"can't\": \"cannot\",\n  \"can't've\": \"cannot have\",\n  \"'cause\": \"because\",\n  \"could've\": \"could have\",\n  \"couldn't\": \"could not\",\n  \"couldn't've\": \"could not have\",\n  \"didn't\": \"did not\",\n  \"doesn't\": \"does not\",\n  \"don't\": \"do not\",\n  \"dont\": \"do not\",\n  \"hadn't\": \"had not\",\n  \"hadn't've\": \"had not have\",\n  \"hasn't\": \"has not\",\n  \"haven't\": \"have not\",\n  \"he'd\": \"he would\",\n  \"idk\": \"i do not know\",\n  \"he'd've\": \"he would have\",\n  \"he'll\": \"he will\",  \"he'll've\": \"he will have\",\n  \"he's\": \"he is\",\n  \"how'd\": \"how did\",\n  \"how'd'y\": \"how do you\",\n  \"how'll\": \"how will\",\n  \"how's\": \"how is\",\n  \"i'd\": \"i would\",\n  \"i'd've\": \"i would have\",\n  \"i'll\": \"i will\",\n  \"i'll've\": \"i will have\",\n  \"i'm\": \"i am\",\n  \"im\": \"i am\",\n  \"i've\": \"i have\",\n  \"isn't\": \"is not\",\n  \"it'd\": \"it had\",\n  \"it'd've\": \"it would have\",\n  \"it'll\": \"it will\",\n  \"it'll've\": \"it will have\",\n  \"it's\": \"it is\", \"let's\": \"let us\",\n  \"ma'am\": \"madam\",\n  \"mayn't\": \"may not\",\n  \"might've\": \"might have\",\n  \"mightn't\": \"might not\",\n  \"mightn't've\": \"might not have\",\n  \"must've\": \"must have\",\n  \"mustn't\": \"must not\",\n  \"mustn't've\": \"must not have\",\n  \"needn't\": \"need not\",\n  \"needn't've\": \"need not have\",\n  \"o'clock\": \"of the clock\",\n  \"oughtn't\": \"ought not\",\n  \"oughtn't've\": \"ought not have\",\n  \"shan't\": \"shall not\",\n  \"sha'n't\": \"shall not\",\n  \"shan't've\": \"shall not have\",\n  \"she'd\": \"she would\",\n  \"she'd've\": \"she would have\", \"she'll\": \"she will\",\n  \"she'll've\": \"she will have\",\n  \"she's\": \"she is\",\n  \"should've\": \"should have\",\n  \"shouldn't\": \"should not\",\n  \"shouldn't've\": \"should not have\",\n  \"so've\": \"so have\",\n  \"so's\": \"so is\",\n  \"that'd\": \"that would\",\n  \"that'd've\": \"that would have\",\n  \"that's\": \"that is\",\n  \"there'd\": \"there had\",\n  \"there'd've\": \"there would have\",\n  \"there's\": \"there is\",\n  \"they'd\": \"they would\",\n  \"they'd've\": \"they would have\",\n  \"they'll\": \"they will\",\n  \"they'll've\": \"they will have\",\n  \"they're\": \"they are\",\n  \"they've\": \"they have\",      \"to've\": \"to have\",\n  \"wasn't\": \"was not\",\n  \"we'd\": \"we had\",\n  \"we'd've\": \"we would have\",\n  \"we'll\": \"we will\",\n  \"we'll've\": \"we will have\",\n  \"we're\": \"we are\",\n  \"we've\": \"we have\",\n  \"weren't\": \"were not\",\n  \"what'll\": \"what will\",\n  \"what'll've\": \"what will have\",\n  \"what're\": \"what are\",\n  \"what's\": \"what is\",\n  \"what've\": \"what have\",\n  \"when's\": \"when is\",\n  \"when've\": \"when have\",\n  \"where'd\": \"where did\",\n  \"where's\": \"where is\",\n  \"where've\": \"where have\", \"who'll\": \"who will\",\n  \"who'll've\": \"who will have\",\n  \"who's\": \"who is\",\n  \"who've\": \"who have\",\n  \"why's\": \"why is\",\n  \"why've\": \"why have\",\n  \"will've\": \"will have\",\n  \"won't\": \"will not\",\n  \"won't've\": \"will not have\",\n  \"would've\": \"would have\",\n  \"wouldn't\": \"would not\",\n  \"wouldn't've\": \"would not have\",\n  \"y'all\": \"you all\",\n  \"y'alls\": \"you alls\",\n  \"y'all'd\": \"you all would\",\n  \"y'all'd've\": \"you all would have\",\n  \"y'all're\": \"you all are\",\n  \"y'all've\": \"you all have\",\n  \"you'd\": \"you had\",\n  \"you'd've\": \"you would have\", \"you'll\": \"you you will\",\n  \"you'll've\": \"you you will have\",\n  \"you're\": \"you are\",\n  \"you've\": \"you have\"\n}\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"abb_re = re.compile('(%s)' % '|'.join(abb.keys()))\n\ndef expandContractions(text, abb_re=abb_re):\n    def replace(match):\n        return abb[match.group(0)]\n    return abb_re.sub(replace, text)\n\ndf['text'] = df['text'].apply(expandContractions)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head(20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Tokenization","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport nltk\nfrom nltk.tokenize import word_tokenize\nnltk.download('punkt')\n\n# Function to tokenize text\ndef tokenize_text(text):\n    tokens = word_tokenize(text)\n    return tokens\n\ndf['text'] = df['text'].apply(tokenize_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head(20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Word Segementation","metadata":{}},{"cell_type":"code","source":"!pip install wordninja","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport wordninja\nimport re\n\n# Apply word segmentation tot he 'text' column in the DataFrame\ndf['text'] = df['text'].apply(lambda tokens: wordninja.split(\" \".join(tokens)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Stopwords Removal","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\n\n# Download the list of English stopwords\nnltk.download('stopwords')\n\n# Retrieve the English stopwords\nstop_words = set(stopwords.words('english'))\n\n# Function to remove stopwords from a list of tokens\ndef remove_stopwords(tokens):\n    return [word for word in tokens if word.lower() not in stop_words]\n\n# Apply the stopword removal function to the 'text' column\ndf['text'] = df['text'].apply(remove_stopwords)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head(10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Punctuation and Digit Removal","metadata":{}},{"cell_type":"code","source":"import string\n\n# Function to remove punctuation and digits\ndef remove_punctuation(tokens):\n    return [word for word in tokens if word not in string.punctuation and not word.isdigit()]\n\n# Apply the function to remove punctuation and digits from the 'text' column\ndf['text'] = df['text'].apply(remove_punctuation)\n\n# Display the first few rows of the 'text' column after removal\nprint(df['text'].head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head(20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Special Characters Removal","metadata":{}},{"cell_type":"code","source":"def remove_special_characters(tokens):\n    # Remove special characters from each token using a regular expression\n    return [re.sub(r'[^a-zA-Z0-9\\s]', '', word) for word in tokens]\n\n# Apply the function to remove special characters\ndf['text'] = df['text'].apply(remove_special_characters)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head(10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Extra white spaces Removal","metadata":{}},{"cell_type":"code","source":"# Function to remove extra whitespaces\ndef remove_extra_whitespaces(tokens):\n    # Strip leading and trailing whitespaces and filter out empty strings\n    return [word.strip() for word in tokens if word.strip() != '']\n\ndf['text'] = df['text'].apply(remove_extra_whitespaces)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head(20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Emoji, mail, and url removal","metadata":{}},{"cell_type":"code","source":"import re\n\n# Function to remove URLs \ndef remove_url(tokens):\n    cleaned_tokens = [re.sub(r'http\\S+', '', token) for token in tokens]\n    return cleaned_tokens\n\n# Function to remove email addresses \ndef remove_mail(tokens):\n    cleaned_tokens = [re.sub(r'\\S+@\\S+', '', token) for token in tokens]\n    return cleaned_tokens\n\n# Function to remove emojis \ndef remove_emoji(tokens):\n    cleaned_tokens = [re.sub(r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U0001FB00-\\U0001FBFF\\U0001FE00-\\U0001FE0F\\U0001F004]+', '', token) for token in tokens]\n    return cleaned_tokens\n\ndf['text'] = df['text'].apply(remove_url)\ndf['text'] = df['text'].apply(remove_mail)\ndf['text'] = df['text'].apply(remove_emoji)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head(20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Text Lemmatization","metadata":{}},{"cell_type":"code","source":"# Load the spaCy English model\nimport spacy\nnlp = spacy.load('en_core_web_sm')\n\ndef lemmatize_tokens(tokens):\n    # Join the tokens back into a sentence\n    text = ' '.join(tokens)\n    # Process the text using spaCy\n    doc = nlp(text)\n    # Lemmatize each token and return the lemmatized tokens\n    lemmatized_tokens = [token.lemma_ for token in doc]\n    return lemmatized_tokens\n\n# Apply lemmatization t\ndf['text'] = df['text'].apply(lemmatize_tokens)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head(20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Remove non-English Words","metadata":{}},{"cell_type":"markdown","source":"we keep some of the words that maybe not include in english word of nltk that we think important for the model features","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import words\nimport nltk\n\n# Download the list of English words (if not already downloaded)\nnltk.download('words')\n\n# Load the set of English words\nenglish_words = set(words.words())\n\n# List of words to exclude from removal\nwords_to_exclude =  {\n    'fuck','suicidal', 'depressed', 'anxiety', 'selfharm', 'overdose', 'hopeless',\n    'cutting', 'lifeless', 'worthless', 'painful', 'enditall', 'sadness',\n    'goodbye', 'helpme', 'hurtmyself', 'numb', 'alone', 'darkness', 'dying',\n    'unloved', 'lost', 'killmyself', 'hanging', 'drugs', 'triggered',\n    'relapse', 'cut', 'sh', 'kms', 'plshelp', 'wanttodie', 'tiredoflife'\n}\n\n# Function to remove non-English words from a list of tokens\ndef remove_non_english(tokens):\n    english_tokens = [\n        token if (token in english_words or token in words_to_exclude) else ''\n        for token in tokens]\n    return [token for token in english_tokens if token != '']\n\n# Apply the function to the 'text' column in the DataFrame\ndf['text'] = df['text'].apply(remove_non_english)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head(20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Modelling","metadata":{}},{"cell_type":"markdown","source":"### Tokenization with keras","metadata":{}},{"cell_type":"code","source":"pip install gensim --no-deps\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport gensim.downloader as api\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, Dropout, Dense\nfrom keras.layers import GRU\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, GRU, Dense, Dropout, Bidirectional, Input\n\n# Configuration\nEMBEDDING_DIM = 100  # Dimension of word embeddings\nMAX_NUM_WORDS = 10000  # Maximum number of unique words\nMAX_SEQUENCE_LENGTH = 200  # Fixed sequence length for all input data\n\n# Combine tokens back into a string to process with the Tokenizer\ndf['text'] = df['text'].apply(lambda tokens: \" \".join(tokens))\n\n# Tokenizer: Train and convert text to sequences\ntokenizer = Tokenizer(num_words=MAX_NUM_WORDS, lower=True)  # Limit vocabulary size and convert to lowercase\ntokenizer.fit_on_texts(df['text'])  # Learn the vocabulary from the text data\nX = tokenizer.texts_to_sequences(df['text'])  # Convert text to numerical sequences\n\n# Pad sequences to ensure uniform length\nX = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH, padding='post')  # Add padding at the end of sequences\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head(20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Convert label to one-hot encoding","metadata":{}},{"cell_type":"code","source":"# Convert labels to one-hot encoding\ny = pd.get_dummies(df['class']).values  # Transform class labels into a binary matrix\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n# `test_size=0.3` allocates 30% of the data for testing, 70% for training\n# `random_state=42` ensures reproducibility of the split\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Word Embedding","metadata":{}},{"cell_type":"code","source":"# Download and load pre-trained GloVe embeddings (100 dimensions)\nglove_gensim = api.load('glove-wiki-gigaword-100')\n\n# Create a weight matrix for the embedding layer\ngensim_weight_matrix = np.zeros((MAX_NUM_WORDS, EMBEDDING_DIM))  # Initialize with zeros\n\n# Populate the weight matrix with GloVe vectors for words in the tokenizer's vocabulary\nfor word, index in tokenizer.word_index.items():\n    if index < MAX_NUM_WORDS:  # Ensure the index is within the specified maximum word limit\n        if word in glove_gensim.index_to_key:  # Check if the word exists in the GloVe vocabulary\n            gensim_weight_matrix[index] = glove_gensim[word]  # Assign the GloVe vector\n        else:\n            gensim_weight_matrix[index] = np.zeros(EMBEDDING_DIM)  # Assign a zero vector if the word is not found\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model Building","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, GRU, Dense, Dropout, Input, BatchNormalization\n\n# Build the model\nmodel = Sequential()\n\n# Input layer\nmodel.add(Input(shape=(X.shape[1],)))\n\n# Embedding layer with pre-trained GloVe embeddings\nmodel.add(Embedding(input_dim=MAX_NUM_WORDS,\n                    output_dim=EMBEDDING_DIM,\n                    weights=[gensim_weight_matrix],\n                    trainable=False)) \n\n# Dropout layer\nmodel.add(Dropout(0.2))\n\n# GRU layer 1\nmodel.add(GRU(100, return_sequences=True))\n\n# Dropout layer\nmodel.add(Dropout(0.2))\n\n# GRU layer 2\nmodel.add(GRU(100, return_sequences=True))\n\n# Dropout layer\nmodel.add(Dropout(0.2))\n\n# GRU layer 3\nmodel.add(GRU(100, return_sequences=False))\n\n# BatchNormalization layer\nmodel.add(BatchNormalization())\n\n# Dropout layer\nmodel.add(Dropout(0.3))\n\n# Dense layer\nmodel.add(Dense(64, activation='relu'))\n\n# Dropout layer\nmodel.add(Dropout(0.3))\n\n# Output layer\nmodel.add(Dense(y.shape[1], activation='sigmoid'))\n\n# Compile the model\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\n# EarlyStopping and ModelCheckpoint\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\nmc = ModelCheckpoint('./best_model.keras', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n\n# Display model summary\nmodel.summary()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Train the Model","metadata":{}},{"cell_type":"code","source":"# Model Training\nhistory = model.fit(X_train, y_train,\n                    validation_split=0.2,\n                    epochs=20,\n                    batch_size=64,\n                    callbacks=[es, mc],\n                    verbose=1)\n\n# Model Evaluation\nloss, accuracy = model.evaluate(X_test, y_test, verbose=1)\nprint(f\"Test Loss: {loss}\")\nprint(f\"Test Accuracy: {accuracy}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model Evaluation","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot for accuracy\nplt.figure(figsize=(12, 5))\n\n# Subplot for accuracy\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Train Accuracy')  # Training accuracy\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')  # Validation accuracy\nplt.title('Accuracy per Epoch')  # Title of the plot\nplt.xlabel('Epochs')  # X-axis label\nplt.ylabel('Accuracy')  # Y-axis label\nplt.legend()  # Display legend\n\n# Subplot for loss\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Train Loss')  # Training loss\nplt.plot(history.history['val_loss'], label='Validation Loss')  # Validation loss\nplt.title('Loss per Epoch')  # Title of the plot\nplt.xlabel('Epochs')  # X-axis label\nplt.ylabel('Loss')  # Y-axis label\nplt.legend()  # Display legend\n\n# Display the plot\nplt.tight_layout()  # Adjust layout for better fit\nplt.show()  # Show the plots\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\n# Get model predictions\ny_pred = model.predict(X_test)\n\n# Convert predictions and labels from one-hot encoding to integer labels\ny_pred_labels = np.argmax(y_pred, axis=1)\ny_test_labels = np.argmax(y_test, axis=1)\n\n# Generate the classification report\nreport = classification_report(y_test_labels, y_pred_labels, target_names=df['class'].unique())\n\n# Print the classification report\nprint(report)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}