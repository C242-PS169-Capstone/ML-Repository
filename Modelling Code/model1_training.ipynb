{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the file\n",
    "file_path = 'Dataset_Suicidal_Sentiment.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Post</th>\n",
       "      <th>Suicidal_label</th>\n",
       "      <th>Sentiment_label</th>\n",
       "      <th>text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Ex Wife Threatening SuicideRecently I left my ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Am I weird I don t get affected by compliments...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Finally   is almost over  So I can never hear ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i need helpjust help me im crying so hard</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I m so lostHello  my name is Adam   and I ve b...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               Post  \\\n",
       "0           0  Ex Wife Threatening SuicideRecently I left my ...   \n",
       "1           1  Am I weird I don t get affected by compliments...   \n",
       "2           2  Finally   is almost over  So I can never hear ...   \n",
       "3           3          i need helpjust help me im crying so hard   \n",
       "4           4  I m so lostHello  my name is Adam   and I ve b...   \n",
       "\n",
       "   Suicidal_label  Sentiment_label  text_length  \n",
       "0               0                0          146  \n",
       "1               1                1           29  \n",
       "2               1                0           25  \n",
       "3               0                0            9  \n",
       "4               0                0          452  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(file_path)\n",
    "# Calculate the length of each post in terms of words\n",
    "df['text_length'] = df['Post'].apply(lambda x: len(str(x).split()))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Suicidal_label\n",
       "0    113534\n",
       "1    113419\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Suicidal_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    226953.000000\n",
       "mean        136.649355\n",
       "std         221.631959\n",
       "min           1.000000\n",
       "25%          27.000000\n",
       "50%          63.000000\n",
       "75%         161.000000\n",
       "max        9685.000000\n",
       "Name: text_length, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 226953 entries, 0 to 226952\n",
      "Data columns (total 5 columns):\n",
      " #   Column           Non-Null Count   Dtype \n",
      "---  ------           --------------   ----- \n",
      " 0   Unnamed: 0       226953 non-null  int64 \n",
      " 1   Post             226895 non-null  object\n",
      " 2   Suicidal_label   226953 non-null  int64 \n",
      " 3   Sentiment_label  226953 non-null  int64 \n",
      " 4   text_length      226953 non-null  int64 \n",
      "dtypes: int64(4), object(1)\n",
      "memory usage: 8.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 226895 entries, 0 to 226952\n",
      "Data columns (total 5 columns):\n",
      " #   Column           Non-Null Count   Dtype \n",
      "---  ------           --------------   ----- \n",
      " 0   Unnamed: 0       226895 non-null  int64 \n",
      " 1   Post             226895 non-null  object\n",
      " 2   Suicidal_label   226895 non-null  int64 \n",
      " 3   Sentiment_label  226895 non-null  int64 \n",
      " 4   text_length      226895 non-null  int64 \n",
      "dtypes: int64(4), object(1)\n",
      "memory usage: 10.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_cleaned = df.dropna()\n",
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download the stopwords from NLTK\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get the list of English stop words\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove stop words from a given text\n",
    "def remove_stop_words(text):\n",
    "    # Split the text into words\n",
    "    words = str(text).split()\n",
    "\n",
    "    # Filter out stop words\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "    # Join the words back into a single string\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply the function to your dataset\n",
    "df_cleaned.loc[:, 'Post'] = df_cleaned['Post'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset contains 226895 examples\n",
      "\n",
      "Text of second example look like this: weird get affected compliments coming someone know irl feel really good internet strangers\n",
      "\n",
      "Labels of first 5 examples look like this: [np.int64(0), np.int64(1), np.int64(1), np.int64(0), np.int64(0)]\n"
     ]
    }
   ],
   "source": [
    "# Since the original dataset does not provide headers you need to index the columns by their index\n",
    "text = df_cleaned['Post'].to_numpy()\n",
    "labels = df_cleaned['Suicidal_label'].to_numpy()\n",
    "\n",
    "# Create the dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((text, labels))\n",
    "\n",
    "# Get the first 5 elements of the dataset\n",
    "examples = list(dataset.take(5))\n",
    "\n",
    "print(f\"dataset contains {len(dataset)} examples\\n\")\n",
    "\n",
    "print(f\"Text of second example look like this: {examples[1][0].numpy().decode('utf-8')}\\n\")\n",
    "print(f\"Labels of first 5 examples look like this: {[x[1].numpy() for x in examples]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "MAX_LENGTH = 150\n",
    "TRAINING_SPLIT = 0.7\n",
    "VAL_SPLIT = 0.2\n",
    "TEST_SPLIT = 0.1\n",
    "NUM_BATCHES = 64\n",
    "PREFETCH_BUFFER_SIZE = tf.data.AUTOTUNE\n",
    "SHUFFLE_BUFFER_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_datasets(dataset):\n",
    "  train_size = int(TRAINING_SPLIT * len(list(dataset)))\n",
    "  val_size = int(VAL_SPLIT * len(list(dataset)))\n",
    "\n",
    "  print(train_size)\n",
    "  print(val_size)\n",
    "\n",
    "  train_dataset = dataset.take(train_size)\n",
    "  remaining = dataset.skip(train_size)\n",
    "  val_dataset = remaining.take(val_size)\n",
    "  test_dataset = remaining.skip(val_size)\n",
    "\n",
    "  # Turn the dataset into a batched dataset with num_batches batches\n",
    "  train_dataset = train_dataset.cache().shuffle(SHUFFLE_BUFFER_SIZE).prefetch(PREFETCH_BUFFER_SIZE).batch(NUM_BATCHES)\n",
    "  val_dataset = val_dataset.cache().shuffle(SHUFFLE_BUFFER_SIZE).prefetch(PREFETCH_BUFFER_SIZE).batch(NUM_BATCHES)\n",
    "  test_dataset = test_dataset.cache().shuffle(SHUFFLE_BUFFER_SIZE).prefetch(PREFETCH_BUFFER_SIZE).batch(NUM_BATCHES)\n",
    "\n",
    "  return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158826\n",
      "45379\n",
      "(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))\n",
      "There are 2482 batches for a total of 158848 elements for training.\n",
      "\n",
      "There are 710 batches for a total of 45440 elements for validation.\n",
      "\n",
      "There are 355 batches for a total of 22720 elements for validation.\n",
      "\n",
      "Total elements in dataset: 227008\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset, test_dataset = split_datasets(dataset)\n",
    "print(train_dataset.element_spec)\n",
    "print(f\"There are {len(train_dataset)} batches for a total of {NUM_BATCHES*len(train_dataset)} elements for training.\\n\")\n",
    "print(f\"There are {len(val_dataset)} batches for a total of {NUM_BATCHES*len(val_dataset)} elements for validation.\\n\")\n",
    "print(f\"There are {len(test_dataset)} batches for a total of {NUM_BATCHES*len(test_dataset)} elements for validation.\\n\")\n",
    "\n",
    "print(f\"Total elements in dataset: {(NUM_BATCHES*len(train_dataset)) + (NUM_BATCHES*len(val_dataset)) + (NUM_BATCHES*len(test_dataset))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_vectorizer(dataset):\n",
    "    # Instantiate the TextVectorization class, defining the necessary arguments alongside their corresponding values\n",
    "    vectorizer = tf.keras.layers.TextVectorization(\n",
    "        standardize='lower_and_strip_punctuation',\n",
    "        output_sequence_length=MAX_LENGTH\n",
    "    )\n",
    "\n",
    "    # Fit the tokenizer to the training sentences\n",
    "    vectorizer.adapt(dataset)\n",
    "\n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary contains 122113 words\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get only the texts out of the dataset\n",
    "text_only_dataset = train_dataset.map(lambda text, label: text)\n",
    "\n",
    "# Adapt the vectorizer to the training sentences\n",
    "vectorizer = fit_vectorizer(text_only_dataset)\n",
    "\n",
    "# Check size of vocabulary\n",
    "vocab_size = vectorizer.vocabulary_size()\n",
    "\n",
    "print(f\"Vocabulary contains {vocab_size} words\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(None, None), dtype=tf.int64, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))\n",
      "Text batch shape: (64, 150)\n",
      "Label batch shape: (64,)\n"
     ]
    }
   ],
   "source": [
    "# Apply vectorization to train and val datasets\n",
    "train_dataset_vectorized = train_dataset.map(lambda x,y: (vectorizer(x), y))\n",
    "val_dataset_vectorized = val_dataset.map(lambda x,y: (vectorizer(x), y))\n",
    "print(train_dataset_vectorized.element_spec)\n",
    "for text_batch, label_batch in train_dataset_vectorized.take(1):\n",
    "    print(f\"Text batch shape: {text_batch.shape}\")\n",
    "    print(f\"Label batch shape: {label_batch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size):\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.Input(shape=(MAX_LENGTH,)),\n",
    "        tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM),\n",
    "        tf.keras.layers.Conv1D(32, 5, activation='relu'),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(0.01))),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64)),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer='rmsprop',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    # Define the correct function signature for on_epoch_end method\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "\n",
    "        # Check if the accuracy is greater or equal to 0.95\n",
    "        if  logs['val_accuracy']>= 0.95:\n",
    "\n",
    "            # Stop training once the above condition is met\n",
    "            self.model.stop_training = True\n",
    "\n",
    "            print(\"\\nReached 95% accuracy so cancelling training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your untrained model\n",
    "model = create_model(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(None, None), dtype=tf.int64, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))\n",
      "predictions have shape: (64, 1)\n"
     ]
    }
   ],
   "source": [
    "# Take an example batch of data\n",
    "example_batch = train_dataset_vectorized.take(1)\n",
    "print(example_batch.element_spec)\n",
    "try:\n",
    "\tmodel.evaluate(example_batch, verbose=False)\n",
    "except:\n",
    "\tprint(\"Your model is not compatible with the dataset you defined earlier. Check that the loss function and last layer are compatible with one another.\")\n",
    "else:\n",
    "\tpredictions = model.predict(example_batch, verbose=False)\n",
    "\tprint(f\"predictions have shape: {predictions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2482/2482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1316s\u001b[0m 514ms/step - accuracy: 0.8731 - loss: 0.3992 - val_accuracy: 0.9353 - val_loss: 0.1854\n",
      "Epoch 2/10\n",
      "\u001b[1m2482/2482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2456s\u001b[0m 990ms/step - accuracy: 0.9366 - loss: 0.1765 - val_accuracy: 0.9377 - val_loss: 0.1723\n",
      "Epoch 3/10\n",
      "\u001b[1m2482/2482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1169s\u001b[0m 471ms/step - accuracy: 0.9457 - loss: 0.1544 - val_accuracy: 0.9371 - val_loss: 0.1763\n",
      "Epoch 4/10\n",
      "\u001b[1m2482/2482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10515s\u001b[0m 4s/step - accuracy: 0.9528 - loss: 0.1373 - val_accuracy: 0.9409 - val_loss: 0.1675\n",
      "Epoch 5/10\n",
      "\u001b[1m2482/2482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1158s\u001b[0m 451ms/step - accuracy: 0.9606 - loss: 0.1197 - val_accuracy: 0.9411 - val_loss: 0.1694\n",
      "Epoch 6/10\n",
      "\u001b[1m2482/2482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2186s\u001b[0m 881ms/step - accuracy: 0.9669 - loss: 0.1029 - val_accuracy: 0.9396 - val_loss: 0.1885\n",
      "Epoch 7/10\n",
      "\u001b[1m2482/2482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1611s\u001b[0m 649ms/step - accuracy: 0.9724 - loss: 0.0887 - val_accuracy: 0.9334 - val_loss: 0.2097\n",
      "Epoch 8/10\n",
      "\u001b[1m2482/2482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1987s\u001b[0m 787ms/step - accuracy: 0.9773 - loss: 0.0749 - val_accuracy: 0.9352 - val_loss: 0.2026\n",
      "Epoch 9/10\n",
      "\u001b[1m2482/2482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1147s\u001b[0m 462ms/step - accuracy: 0.9808 - loss: 0.0639 - val_accuracy: 0.9319 - val_loss: 0.2257\n",
      "Epoch 10/10\n",
      "\u001b[1m2482/2482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1052s\u001b[0m 424ms/step - accuracy: 0.9848 - loss: 0.0540 - val_accuracy: 0.9255 - val_loss: 0.3082\n"
     ]
    }
   ],
   "source": [
    "# Train the model and save the training history\n",
    "history = model.fit(\n",
    "\ttrain_dataset_vectorized,\n",
    "\tepochs=10,\n",
    "\tvalidation_data=val_dataset_vectorized,\n",
    "\tcallbacks=[EarlyStoppingCallback()],\n",
    "  verbose=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
